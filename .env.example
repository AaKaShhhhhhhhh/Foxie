# Foxie Desktop AI Pet - Environment Configuration

# LLM Provider
# - `tambo` uses Tambo Threads API (recommended)
# - `openai` uses OpenAI Chat Completions API
# Note: In the Electron desktop build, `LLM_PROVIDER` is the source of truth.
# For browser-only runs (`npm run dev` / `npm run preview`), `VITE_LLM_PROVIDER` is used.
# Keep them in sync unless you intentionally want different behavior.
LLM_PROVIDER=tambo

# Renderer-side provider selector (defaults to `tambo`)
VITE_LLM_PROVIDER=tambo
VITE_LLM_PROXY_URL=/ask

# Tambo (server-side) configuration
# Used by:
# - Electron main process (IPC LLM proxy)
# - Vite dev/preview server `/ask` proxy (so the key isn't exposed to the browser)
# This key must be a Tambo project API key.
TAMBO_API_KEY=your_tambo_project_api_key_here
TAMBO_THREAD_ID=thr_your_thread_id_here
TAMBO_API_BASE_URL=https://api.tambo.co

# Optional: OpenAI (only when `LLM_PROVIDER=openai`)
# OPENAI_API_KEY=your_openai_api_key_here

# Optional: Audio Settings
VITE_ENABLE_AUDIO=true
VITE_AUDIO_VOLUME=0.7
